# OpsConductor NMS Environment Configuration
# Copy this file to .env and adjust values as needed

# ============================================================================
# LLM Configuration
# ============================================================================

# Model to use for topology inference
# Options: 
#   - microsoft/Phi-3-mini-4k-instruct (recommended, 4K context)
#   - microsoft/Phi-3-mini-128k-instruct (larger context)
MODEL_NAME=microsoft/Phi-3-mini-4k-instruct

# Maximum context length for vLLM
# Reduce this if you experience OOM errors
# Range: 2048-4096 for Phi-3-mini-4k
VLLM_MAX_CONTEXT_LEN=4096

# HuggingFace Hub Token (optional)
# Required only for private/gated models
# Get yours at: https://huggingface.co/settings/tokens
HF_TOKEN=

# Local model directory (optional)
# If you've pre-downloaded the model, specify the path
# MODEL_DIR=./models

# ============================================================================
# Network Configuration
# ============================================================================

# Known gateway IP (optional seed fact)
# This helps the LLM bootstrap topology discovery
GATEWAY_IP=

# Known firewall IP (optional seed fact)
FIREWALL_IP=

# ============================================================================
# Analyst Service Configuration
# ============================================================================

# Evidence batch window size in milliseconds
# Default: 250ms (4 batches per second)
BATCH_MS=250

# Maximum combined ARP + flow items per evidence window
# Prevents overwhelming the LLM with too much data
MAX_EVIDENCE_ITEMS=512

# LLM response format
# Options: json_schema (strict), json_object (flexible)
RESPONSE_FORMAT=json_schema

# ============================================================================
# State Server Configuration
# ============================================================================

# WebSocket CORS origin
# Default: * (allow all)
# Production: Set to specific origin (e.g., https://topology.example.com)
UI_WS_ORIGIN=*

# Database connection string
# Default is set in docker-compose.yml
# DB_URL=postgresql://topo:topo@postgres:5432/topology

# ============================================================================
# UI Configuration
# ============================================================================

# API base URL for the UI
# Update this if accessing from a different machine
# Format: http://hostname:port (no trailing slash)
VITE_API_BASE=http://localhost:8080

# WebSocket base URL for the UI
# Format: ws://hostname:port (no trailing slash)
VITE_WS_BASE=ws://localhost:8080

# ============================================================================
# Development & Debugging
# ============================================================================

# Enable SQL query logging (state-server)
# SQL_ECHO=true

# Log level for services
# LOG_LEVEL=INFO
